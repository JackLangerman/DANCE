{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if '/scratch/zq415/grammar_cor/pose/pose_estimation_unified/src' in sys.path:\n",
    "    sys.path.remove('/scratch/zq415/grammar_cor/pose/pose_estimation_unified/src')\n",
    "\n",
    "import time\n",
    "import torch\n",
    "from options.train_options import TrainOptions\n",
    "from data import create_dataset\n",
    "from models import create_model\n",
    "from util.visualizer import Visualizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:{}'.format(0))\n",
    "netG = networks.define_G(3, 3, 64, 'resnet_9blocks', 'instance', False, 'normal', 0.02, [0])\n",
    "state_dict = torch.load('./checkpoints/render2real_cyclegan2/latest_net_G_A.pth', map_location=str(device))\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    if 'module.' not in k:\n",
    "        k = 'module.'+ k\n",
    "        new_state_dict[k]=v\n",
    "        \n",
    "netG.load_state_dict(new_state_dict)\n",
    "netG.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rendered_path = '/scratch/zq415/grammar_cor/pose/pose_estimation_unified/data/data_collection/robot_lab_03_13_2020_000/rendered_images'\n",
    "render_img_paths, render_scr_paths, render_poses = make_render_dataset(rendered_path)\n",
    "print(len(render_img_paths), len(render_scr_paths), render_poses.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset_scr(render_img_paths[:1000], render_poses, render_scr_paths,\n",
    "                        transform=transforms.Compose([random_crop()]))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1,\n",
    "                            shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG.eval()\n",
    "for i_batch, sample_batched in enumerate(train_dataloader):\n",
    "    inputs, scrs, poses = (sample_batched['image']-0.5)/0.5, sample_batched['scr'], sample_batched['pose']\n",
    "    labels = sample_batched['scr']\n",
    "    inputs = inputs.to(device, dtype=torch.float)\n",
    "    with torch.no_grad():\n",
    "        fake_B = netG(inputs)\n",
    "    save_image_test(fake_B, './results', str(i_batch)+'_fake_real')\n",
    "    save_image_test(inputs, './results', str(i_batch)+'_render')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
